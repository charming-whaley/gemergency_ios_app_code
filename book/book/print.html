<!DOCTYPE HTML>
<html lang="en" class="light sidebar-visible" dir="ltr">
    <head>
        <!-- Book generated using mdBook -->
        <meta charset="UTF-8">
        <title>Gemergency iOS app docs</title>
        <meta name="robots" content="noindex">


        <!-- Custom HTML head -->

        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">
        <meta name="theme-color" content="#ffffff">

        <link rel="icon" href="favicon.svg">
        <link rel="shortcut icon" href="favicon.png">
        <link rel="stylesheet" href="css/variables.css">
        <link rel="stylesheet" href="css/general.css">
        <link rel="stylesheet" href="css/chrome.css">
        <link rel="stylesheet" href="css/print.css" media="print">

        <!-- Fonts -->
        <link rel="stylesheet" href="FontAwesome/css/font-awesome.css">
        <link rel="stylesheet" href="fonts/fonts.css">

        <!-- Highlight.js Stylesheets -->
        <link rel="stylesheet" id="highlight-css" href="highlight.css">
        <link rel="stylesheet" id="tomorrow-night-css" href="tomorrow-night.css">
        <link rel="stylesheet" id="ayu-highlight-css" href="ayu-highlight.css">

        <!-- Custom theme stylesheets -->


        <!-- Provide site root and default themes to javascript -->
        <script>
            const path_to_root = "";
            const default_light_theme = "light";
            const default_dark_theme = "navy";
            window.path_to_searchindex_js = "searchindex.js";
        </script>
        <!-- Start loading toc.js asap -->
        <script src="toc.js"></script>
    </head>
    <body>
    <div id="mdbook-help-container">
        <div id="mdbook-help-popup">
            <h2 class="mdbook-help-title">Keyboard shortcuts</h2>
            <div>
                <p>Press <kbd>←</kbd> or <kbd>→</kbd> to navigate between chapters</p>
                <p>Press <kbd>S</kbd> or <kbd>/</kbd> to search in the book</p>
                <p>Press <kbd>?</kbd> to show this help</p>
                <p>Press <kbd>Esc</kbd> to hide this help</p>
            </div>
        </div>
    </div>
    <div id="body-container">
        <!-- Work around some values being stored in localStorage wrapped in quotes -->
        <script>
            try {
                let theme = localStorage.getItem('mdbook-theme');
                let sidebar = localStorage.getItem('mdbook-sidebar');

                if (theme.startsWith('"') && theme.endsWith('"')) {
                    localStorage.setItem('mdbook-theme', theme.slice(1, theme.length - 1));
                }

                if (sidebar.startsWith('"') && sidebar.endsWith('"')) {
                    localStorage.setItem('mdbook-sidebar', sidebar.slice(1, sidebar.length - 1));
                }
            } catch (e) { }
        </script>

        <!-- Set the theme before any content is loaded, prevents flash -->
        <script>
            const default_theme = window.matchMedia("(prefers-color-scheme: dark)").matches ? default_dark_theme : default_light_theme;
            let theme;
            try { theme = localStorage.getItem('mdbook-theme'); } catch(e) { }
            if (theme === null || theme === undefined) { theme = default_theme; }
            const html = document.documentElement;
            html.classList.remove('light')
            html.classList.add(theme);
            html.classList.add("js");
        </script>

        <input type="checkbox" id="sidebar-toggle-anchor" class="hidden">

        <!-- Hide / unhide sidebar before it is displayed -->
        <script>
            let sidebar = null;
            const sidebar_toggle = document.getElementById("sidebar-toggle-anchor");
            if (document.body.clientWidth >= 1080) {
                try { sidebar = localStorage.getItem('mdbook-sidebar'); } catch(e) { }
                sidebar = sidebar || 'visible';
            } else {
                sidebar = 'hidden';
                sidebar_toggle.checked = false;
            }
            if (sidebar === 'visible') {
                sidebar_toggle.checked = true;
            } else {
                html.classList.remove('sidebar-visible');
            }
        </script>

        <nav id="sidebar" class="sidebar" aria-label="Table of contents">
            <!-- populated by js -->
            <mdbook-sidebar-scrollbox class="sidebar-scrollbox"></mdbook-sidebar-scrollbox>
            <noscript>
                <iframe class="sidebar-iframe-outer" src="toc.html"></iframe>
            </noscript>
            <div id="sidebar-resize-handle" class="sidebar-resize-handle">
                <div class="sidebar-resize-indicator"></div>
            </div>
        </nav>

        <div id="page-wrapper" class="page-wrapper">

            <div class="page">
                <div id="menu-bar-hover-placeholder"></div>
                <div id="menu-bar" class="menu-bar sticky">
                    <div class="left-buttons">
                        <label id="sidebar-toggle" class="icon-button" for="sidebar-toggle-anchor" title="Toggle Table of Contents" aria-label="Toggle Table of Contents" aria-controls="sidebar">
                            <i class="fa fa-bars"></i>
                        </label>
                        <button id="theme-toggle" class="icon-button" type="button" title="Change theme" aria-label="Change theme" aria-haspopup="true" aria-expanded="false" aria-controls="theme-list">
                            <i class="fa fa-paint-brush"></i>
                        </button>
                        <ul id="theme-list" class="theme-popup" aria-label="Themes" role="menu">
                            <li role="none"><button role="menuitem" class="theme" id="default_theme">Auto</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="light">Light</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="rust">Rust</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="coal">Coal</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="navy">Navy</button></li>
                            <li role="none"><button role="menuitem" class="theme" id="ayu">Ayu</button></li>
                        </ul>
                        <button id="search-toggle" class="icon-button" type="button" title="Search (`/`)" aria-label="Toggle Searchbar" aria-expanded="false" aria-keyshortcuts="/ s" aria-controls="searchbar">
                            <i class="fa fa-search"></i>
                        </button>
                    </div>

                    <h1 class="menu-title">Gemergency iOS app docs</h1>

                    <div class="right-buttons">
                        <a href="print.html" title="Print this book" aria-label="Print this book">
                            <i id="print-button" class="fa fa-print"></i>
                        </a>

                    </div>
                </div>

                <div id="search-wrapper" class="hidden">
                    <form id="searchbar-outer" class="searchbar-outer">
                        <div class="search-wrapper">
                            <input type="search" id="searchbar" name="searchbar" placeholder="Search this book ..." aria-controls="searchresults-outer" aria-describedby="searchresults-header">
                            <div class="spinner-wrapper">
                                <i class="fa fa-spinner fa-spin"></i>
                            </div>
                        </div>
                    </form>
                    <div id="searchresults-outer" class="searchresults-outer hidden">
                        <div id="searchresults-header" class="searchresults-header"></div>
                        <ul id="searchresults">
                        </ul>
                    </div>
                </div>

                <!-- Apply ARIA attributes after the sidebar and the sidebar toggle button are added to the DOM -->
                <script>
                    document.getElementById('sidebar-toggle').setAttribute('aria-expanded', sidebar === 'visible');
                    document.getElementById('sidebar').setAttribute('aria-hidden', sidebar !== 'visible');
                    Array.from(document.querySelectorAll('#sidebar a')).forEach(function(link) {
                        link.setAttribute('tabIndex', sidebar === 'visible' ? 0 : -1);
                    });
                </script>

                <div id="content" class="content">
                    <main>
                        <h1 id="gemergency"><a class="header" href="#gemergency">Gemergency</a></h1>
<h2 id="intro"><a class="header" href="#intro">Intro</a></h2>
<p>We are very excited to present you Gemergency - an iOS help assistant app based on Google Gemma 3n LLM. This project was build for the <a href="https://www.kaggle.com/competitions/google-gemma-3n-hackathon">Google's hackathon "Google - The Gemma 3n Impact Challenge" on Kaggle</a></p>
<h2 id="why-this-project-matters"><a class="header" href="#why-this-project-matters">Why This Project Matters</a></h2>
<p>In moments of crisis, every second counts — but most people freeze or forget what to do.  
We believe that <i>everyone</i> deserves instant, trustworthy help, no matter where they are or what device they use.</p>
<p>Our project transforms Google Gemma 3n into an offline-first AI assistant that guides users step by step through emergencies — from home accidents and natural disasters to health crises — with advice that is clear, safe, and scientifically sound.</p>
<p>Unlike most AI apps, our solution is fully functional <b>without internet</b>. That means you can get lifesaving guidance anywhere:</p>
<ul>
    <li>On a plane</li>
    <li>In the wild</li>
    <li>During blackouts</li>
    <li>Or simply when you're overwhelmed and your mind goes blank</li>
</ul> 
<p>We know that under stress, people don’t read instructions — they need actionable, friendly support. Our AI doesn't just repeat generic advice: it provides concise, situation-specific steps.</p>
<p><b>We are building a world where anyone, anywhere, can get calm, reliable help — even when it feels like there’s no one around.</b><br></p>
<p>Whether you’re treating a deep cut, coping with a fire, or facing a natural disaster, our offline Gemma 3n assistant is always there for you — no signal, no panic, just help.</p>
<h2 id="additional"><a class="header" href="#additional">Additional</a></h2>
<p>In this docs, you will find the info about the model training and app creation process, how it all works. But besides, you can also find some very useful info for your next iOS app project</p>
<p>Besides, don't forget to follow each member of the team Ultra-Ochevness:</p>
<ul>
    <li><a href="https://github.com/Yameteshka">Julia Kurnaeva (@Yameteshka)</a> - ML-engineer, trained Google Gemma 3n</li>
    <li><a href="https://github.com/charming-whaley">Fedya Katkov (@charming-whaley)</a> - iOS developer, made Gemergency iOS app</li>
</ul>
<p>And you can also try Gemergency out by:</p>
<ul>
    <li>Downloading <a href="https://apps.apple.com/us/app/testflight/id899247664">TestFlight</a> from the App Store</li>
    <li>Gainining access to beta test <a href="https://testflight.apple.com/join/uAbAGUsP">Gemergency</a> on TestFlight</li>
</ul>
<div style="break-before: page; page-break-before: always;"></div><h1 id="original-model--tooling-overview"><a class="header" href="#original-model--tooling-overview">Original Model & Tooling Overview</a></h1>
<p>A successful fine-tuning project begins with a deliberate selection of the base model and a robust set of supporting tools. Our choices were guided by a commitment to performance, efficiency, and deployment accessibility.</p><div style="break-before: page; page-break-before: always;"></div><h2 id="base-model-unslothgemma-3n-e2b-it"><a class="header" href="#base-model-unslothgemma-3n-e2b-it">Base Model: <code>unsloth/gemma-3n-E2B-it</code></a></h2>
<p>We selected <b>Gemma</b>, a family of lightweight, state-of-the-art open models from Google, as our foundation. Specifically, we chose the <code>unsloth/gemma-3n-E2B-it</code> variant.</p>
<ul>
    <li><b>Architecture</b>: Gemma models are based on the same decoder-only Transformer architecture as the Gemini models. The "3n" variant is a <b>multimodal</b> model, equipped with a Vision Language Encoder, making it capable of processing both text and image inputs. While this project focuses on text-to-text fine-tuning, the multimodal foundation offers a clear path for future expansion (e.g., analyzing photos of injuries).</li>
    <li><b>Training and Capabilities</b>: The <code>-it</code> suffix signifies that the model is <b>Instruction Tuned</b>. Following its extensive pre-training on a diverse corpus of up to 6 trillion tokens of text and code, it underwent supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) to enhance its ability to follow instructions and engage in helpful dialogue.</li>
    <li><b>Known Limitations</b>: As with all LLMs, Gemma is not immune to generating factually incorrect or biased information. Google's official documentation notes that while safety filters are in place, the model may not always capture nuances and can sometimes produce responses that are plausible but incorrect. This limitation was a primary motivator for our targeted fine-tuning, aiming to instill domain-specific accuracy for emergency scenarios.</li>
</ul>
<p><b>References</b></p>
<ul>
    <li><a href="https://deepmind.google/models/gemma/">Google DeepMind's Gemma</a></li>
    <li><a href="https://ai.google.dev/gemma/docs">Google's documentation</a></li>
    <li><a href="https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune">Unsloth docs</a></li>
</ul><div style="break-before: page; page-break-before: always;"></div><h2 id="justification-for-tooling-choices"><a class="header" href="#justification-for-tooling-choices">Justification for Tooling Choices</a></h2>
<ul>
    <li><b>Unsloth</b>: To overcome the significant computational costs of fine-tuning, we leveraged <b>Unsloth</b>. Unsloth provides highly optimized kernels that enable up to 2x faster training and reduce memory usage by 60% without sacrificing performance. This is achieved through manual autograd functions, re-engineered RoPE embeddings, and other deep optimizations. Its seamless integration (<code>FastModel</code>) allowed us to implement advanced techniques like QLoRA with minimal boilerplate code, making the entire process more efficient and accessible.</li>
    <ul>
        <li><b>Reference</b>: <a href="https://github.com/unslothai/unsloth">Unsloth GitHub Repository</a></li>
    </ul><br>
    <li><b>GGUF and <code>llama.cpp</code></b>: Our end goal is a model that is not only accurate but also deployable in resource-constrained environments. We chose the <b>GGUF (GPT-Generated Unified Format)</b> for this purpose. GGUF is a file format designed by the <code>llama.cpp</code> community for packaging and running LLMs efficiently. It quantizes model weights (reducing precision from 16-bit to as low as 2-bit), drastically shrinking file size and enabling fast inference on CPUs or consumer-grade GPUs. This makes our emergency assistant potentially deployable on edge devices or personal computers, increasing its real-world impact.</li>
    <ul>
        <li><b>Reference</b>: <a href="https://github.com/ggerganov/llama.cpp">llama.cpp GitHub Repository</a></li>
    </ul>
</ul><div style="break-before: page; page-break-before: always;"></div><h1 id="inference-setup"><a class="header" href="#inference-setup">Inference setup</a></h1>
<h2 id="integration-choice"><a class="header" href="#integration-choice">Integration choice</a></h2>
<p>In the beginning, when it comes to building an iOS app with LLM, the developer needs to choose the way it will be integrated in the app. In our case, there were standard ways of using that on-device:</p>
<ul>
    <li><a href="https://pypi.org/project/coremltools/">coremltools</a> from pip</li>
    <li><a href="https://github.com/ggml-org/llama.cpp">llama.cpp inference</a> with .gguf file extension</li>
    <li><a href="https://ai.google.dev/edge/mediapipe/solutions/guide">Google's MediaPipe</a></li>
    <li>Use of <a href="https://onnx.ai/">ONNX</a></li>
</ul>
<p>After some time of working with all these methods, we came across on pros/cons of each of those ways:</p>
<table>
  <tr>
    <th></th>
    <th>coremltools</th>
    <th>llama.cpp</th>
    <th>MediaPipe</th>
    <th>ONNX</th>
  </tr>
  <tr>
    <td>Pros</td>
    <td>Easily integrated via Apple's CoreML</td>
    <td>A developer can gain access to lower-level settings</td>
    <td>Standard way of integrating Google's LLMs</td>
    <td>Use with coremltools by running just one command</td>
  </tr>
  <tr>
    <td>Cons</td>
    <td>Not supported for now [08/03/2025]</td>
    <td>Too hard war for noobs</td>
    <td>Google Gemma 3n is not supported for now [08/03/2025]</td>
    <td>Need for high-performed Mac 16+ of RAM and Apple Silicon Pro+ processors</td>
  </tr>
</table>
<p>Unfortunately, we couldn't use <i>coremltools</i> or <i>ONNX</i>, which are considered as the best tools for using LLMs on iOS, so we narrowed such tools down to <i>llama.cpp</i> and <i>MediaPipe</i>. And, as it often happens, <i>MediaPipe</i> became not appropriate for us because we realized that there is no way to convert Google Gemma 3n into .task file extension. Hence, the only thing we could try is <i>llama.cpp</i></p>
<p>We are going through each LLM integration step in the Gemergency iOS app. We first start with <i>llama.cpp</i> setup and finally go to building our own SwiftUI iOS app</p>
<h2 id="llamacpp-setup"><a class="header" href="#llamacpp-setup">llama.cpp setup</a></h2>
<p>First things first, we had to install <i>llama.cpp</i> inference on macOS. For this, we need to clone the official repo on the Mac:</p>
<pre><code class="language-bash">$ git clone --recursive https://github.com/ggml-org/llama.cpp.git &amp;&amp; cd llama.cpp
</code></pre>
<p>By running that command, we clone and go to the root directory of <i>llama.cpp</i>. We can find <b>example/llama.swiftui</b> subdirectory there. This is what we need. But before going there, we have to build Xcode framework for further use in SwiftUI iOS app. Run this command in the root directory of <i>llama.cpp</i>:</p>
<pre><code class="language-bash">$ ./build-xcframework.sh
</code></pre>
<p>And that's it! We can not proceed by integrating Google Gemma 3n into the iOS app</p><div style="break-before: page; page-break-before: always;"></div><h1 id="setting-up-an-ios-app-with-gemma-3n"><a class="header" href="#setting-up-an-ios-app-with-gemma-3n">Setting up an iOS app with Gemma 3n</a></h1>
<h2 id="preparation"><a class="header" href="#preparation">Preparation</a></h2>
<p>We were ready to build a brand new SwiftUI iOS app. Before we began, we had to set up the <b>Xcode framework</b> within the app. To start, we created a new SwiftUI project in Xcode by navigating to <b>Xcode → File → New → Project</b>, selecting SwiftUI as the primary UI framework, and creating a new app</p>
<p>Next, we had to add the <b>Xcode framework</b> we built earlier to the project. This can be done easily by simply dragging and dropping the framework into our app. Once that's done, we could move on to integrating the necessary controllers into the app</p>
<h2 id="app-setup"><a class="header" href="#app-setup">App setup</a></h2>
<p>To work successfully with Gemma 3n, our SwiftUI iOS app requires two key controllers: <b>LlamaState</b> and <b>LibLlama</b>. Both can be found in <b>llama.cpp/examples/llama.swiftui</b>:</p>
<ul>
    <li><b>LlamaState</b> - acts as a bridge between the SwiftUI app and <i>llama.cpp</i>, using <b>LibLlama</b></li>
    <li><b>LibLlama</b> - serves as the core engine that manages LLM setup within the SwiftUI app</li>
</ul>
<p>After adding these controllers to our SwiftUI project, we were ready to begin designing the app's user interface</p>
<h2 id="additional-features-to-controllers"><a class="header" href="#additional-features-to-controllers">Additional features to controllers</a></h2>
<p>In addition to adding these controllers to the project, we also needed to modify them to ensure they functioned correctly</p>
<p>First things first, we had to add these lines of code into <b>LibLlama</b>:</p>
<pre><code class="language-swift">func clear() {
    tokens_list.removeAll()
    temporary_invalid_cchars.removeAll()
    llama_memory_clear(llama_get_memory(context), true)

    self.n_cur = 0 // &lt;- add this line
    self.is_done = false // &lt;- add this line
}
</code></pre>
<p>Without these lines of code, Gemma 3n won't respond to a second prompt. To clarify: the first prompt works as expected and receives a response, but the second prompt fails because the session cache isn't cleared between prompts.</p>
<pre><code class="language-swift">static func create_context(path: String) throws -&gt; LlamaContext {
    llama_backend_init()
    var model_params = llama_model_default_params() // &lt;- add this line

#if targetEnvironment(simulator)
    model_params.n_gpu_layers = 0
    print("Running on simulator, force use n_gpu_layers = 0")
#endif
    model_params.n_gpu_layers = 0 // &lt;- add this line

    let model = llama_model_load_from_file(path, model_params)
    guard let model else {
        print("Could not load model at \(path)")
        throw LlamaError.couldNotInitializeContext
    }

    let n_threads = max(1, min(8, ProcessInfo.processInfo.processorCount - 2))
    print("Using \(n_threads) threads")

    var ctx_params = llama_context_default_params()
    ctx_params.n_ctx = 2048
    ctx_params.n_threads       = Int32(n_threads)
    ctx_params.n_threads_batch = Int32(n_threads)

    let context = llama_init_from_model(model, ctx_params)
    guard let context else {
        print("Could not load context!")
        throw LlamaError.couldNotInitializeContext
    }

    return LlamaContext(model: model, context: context)
}
</code></pre>
<p>Without these lines of code, the model won't load on physical devices. It may run successfully from within Xcode, it will fail to work — such as when distributed via TestFlight — on any actual device though</p>
<p>Other necessary settings and methods can be found on <a href="https://github.com/charming-whaley/gemergency_ios_app_code">Github repo of Gemergency</a></p>
<h2 id="further-steps"><a class="header" href="#further-steps">Further steps</a></h2>
<p>With that completed, we proceeded to develop the Gemergency iOS app. The next steps involved designing the UI with SwiftUI, integrating iOS system features, and implementing other core functionalities</p>
<div style="break-before: page; page-break-before: always;"></div><h1 id="gemergency-app-publishing"><a class="header" href="#gemergency-app-publishing">Gemergency app publishing</a></h1>
<h2 id="publishing-idea"><a class="header" href="#publishing-idea">Publishing idea</a></h2>
<p>After developing our app, we wanted to make Gemergency easily accessible, so that anyone could download and use it on an iPhone/iPad. However, we quickly ran into two key challenges: where should we publish the app, and how could we get Gemma 3n running on actual devices? It might sound weird, but these were real problems</p>
<p>Let me explain why this matters and what solutions we found:</p>
<ul>
    <li>Where should we publish the app? - there were no time and opportunities to publish the app on the App Store. So had to find another platform where we could distribute Gemergency. There was only one such platform - TestFlight (it's not directly App Store, but the app might still be used by users)</li>
    <li>How could we make Gemma 3n running on physical devices? - since the first Gemergency beta, there was no opportunity to run Gemma 3n on physical device. We found very interesting solution that we are going to explain below though</li>
</ul>
<h2 id="testflight-distribution"><a class="header" href="#testflight-distribution">TestFlight distribution</a></h2>
<p>After choosing the platform which was the TestFlight, we first had to set up our app a bit: change scheme type from <b>Debug</b> to <b>Release</b>, as well as adapt Gemergency for all necessary devices: iPhones with dynamic island, all other iPhones, iPads. After that, we went into the work with model...</p>
<h2 id="problem-with-model"><a class="header" href="#problem-with-model">Problem with model</a></h2>
<p>By default, Gemma 3n used GPU for working on device. And while working on a simulator was swift and smooth due to Apple Silicon CPU line-up, we came across that it does not work even on iPhone 16 Pro Max in real life. That was strange for us, because the newest iPhones' capabilities were made to work AI</p>
<p>We spent about 2 days to get with problem. And accidentally we found how to solve this: we found that in case of simulator, we set GPU layers to 0, so the app works smooth on simulator. But what about physical devices? Well, that' funny but physical devices used GPU layers for work</p>
<p>To change that, we just added one line of code (well, we copied that line from #if directive) in <b>LibLlama</b> controller:</p>
<pre><code class="language-swift">static func create_context(path: String) throws -&gt; LlamaContext {
    llama_backend_init()
    var model_params = llama_model_default_params() // &lt;- add this line

#if targetEnvironment(simulator)
    model_params.n_gpu_layers = 0
    print("Running on simulator, force use n_gpu_layers = 0")
#endif
    model_params.n_gpu_layers = 0 // &lt;- add this line

    let model = llama_model_load_from_file(path, model_params)
    guard let model else {
        print("Could not load model at \(path)")
        throw LlamaError.couldNotInitializeContext
    }

    let n_threads = max(1, min(8, ProcessInfo.processInfo.processorCount - 2))
    print("Using \(n_threads) threads")

    var ctx_params = llama_context_default_params()
    ctx_params.n_ctx = 2048
    ctx_params.n_threads       = Int32(n_threads)
    ctx_params.n_threads_batch = Int32(n_threads)

    let context = llama_init_from_model(model, ctx_params)
    guard let context else {
        print("Could not load context!")
        throw LlamaError.couldNotInitializeContext
    }

    return LlamaContext(model: model, context: context)
}
</code></pre>
<p>And that's it! We are done! Now we could sent our app to TestFlight and distribute it via users all across Apple Ecosystem (but still via the link)</p>
                    </main>

                    <nav class="nav-wrapper" aria-label="Page navigation">
                        <!-- Mobile navigation buttons -->


                        <div style="clear: both"></div>
                    </nav>
                </div>
            </div>

            <nav class="nav-wide-wrapper" aria-label="Page navigation">

            </nav>

        </div>

        <!-- Livereload script (if served using the cli tool) -->
        <script>
            const wsProtocol = location.protocol === 'https:' ? 'wss:' : 'ws:';
            const wsAddress = wsProtocol + "//" + location.host + "/" + "__livereload";
            const socket = new WebSocket(wsAddress);
            socket.onmessage = function (event) {
                if (event.data === "reload") {
                    socket.close();
                    location.reload();
                }
            };

            window.onbeforeunload = function() {
                socket.close();
            }
        </script>



        <script>
            window.playground_copyable = true;
        </script>


        <script src="elasticlunr.min.js"></script>
        <script src="mark.min.js"></script>
        <script src="searcher.js"></script>

        <script src="clipboard.min.js"></script>
        <script src="highlight.js"></script>
        <script src="book.js"></script>

        <!-- Custom JS scripts -->

        <script>
        window.addEventListener('load', function() {
            window.setTimeout(window.print, 100);
        });
        </script>


    </div>
    </body>
</html>
