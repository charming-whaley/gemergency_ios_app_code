{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Pipeline Demonstration (Live Code)\n",
    "\n",
    "Welcome to the live demonstration notebook for the **Gemma 3n Fine-Tuning for Emergency Assistance** project.\n",
    "\n",
    "### The Purpose of This Notebook & An Important Note on the Model\n",
    "\n",
    "The full project, which uses the **multimodal `unsloth/gemma-3n-E2B-it` model**, is designed to run within a Docker container. This is crucial for handling the complex dependencies of its vision components.\n",
    "\n",
    "Replicating this specific environment with `pip` in a live notebook can be unreliable. Therefore, to ensure this demonstration runs flawlessly from end to end, **we will use the text-only `unsloth/gemma-2b-it` model**.\n",
    "\n",
    "This allows us to demonstrate the **entire, identical code pipeline** ‚Äî data processing, LoRA fine-tuning, model merging, and inference ‚Äî without encountering environment-specific errors related to the vision tower. The logic you see here is the same logic used in the main project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Project Architecture Overview\n",
    "\n",
    "Before we begin, here is the professional structure of our project, which this notebook simulates.\n",
    "\n",
    "```\n",
    "gemma_local_trainer/\n",
    "‚îú‚îÄ‚îÄ Dockerfile             # For the main training environment\n",
    "‚îú‚îÄ‚îÄ Dockerfile.convert     # For GGUF conversion\n",
    "‚îú‚îÄ‚îÄ README.md\n",
    "‚îú‚îÄ‚îÄ requirements.txt\n",
    "‚îú‚îÄ‚îÄ data/                  # Full dataset location\n",
    "‚îú‚îÄ‚îÄ models/                # Full model artifact location\n",
    "‚îú‚îÄ‚îÄ scripts/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ convert_to_gguf.py\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ inference_gguf.py\n",
    "‚îî‚îÄ‚îÄ src/\n",
    "    ‚îú‚îÄ‚îÄ __init__.py\n",
    "    ‚îú‚îÄ‚îÄ config.py\n",
    "    ‚îú‚îÄ‚îÄ inference.py\n",
    "    ‚îú‚îÄ‚îÄ train_pipeline.py\n",
    "    ‚îî‚îÄ‚îÄ utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Installing Dependencies\n",
    "\n",
    "This cell will install all necessary Python libraries for this demonstration.\n",
    "\n",
    "*(Note: `%%capture` is used to hide the lengthy installation output.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"TORCH_LOGS\"] = \"+dynamo\"\n",
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "os.environ[\"TORCH_COMPILE_DISABLE\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# We install the necessary packages. Note that the version constraints that worked\n",
    "# in the Dockerfile might conflict with a live notebook's pre-installed packages.\n",
    "# This simpler installation is more robust for a demo environment.\n",
    "!pip install \"unsloth[cu121-ampere-torch23]\" \"transformers\" \"datasets\" \"trl\" \"peft\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Setup and Lightweight Configuration\n",
    "\n",
    "Now, we'll import libraries, define a lightweight configuration for our demo, and create the temporary dataset. All files will be placed in `_demo` directories to avoid cluttering the main project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "Demo environment is set up and ready.\n"
     ]
    }
   ],
   "source": [
    "from unsloth import FastModel, is_bfloat16_supported\n",
    "import gc\n",
    "import shutil\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "# --- DEMO-SPECIFIC Configuration ---\n",
    "# We create temporary directories for this run\n",
    "PROJECT_ROOT = Path(\"./\").resolve()\n",
    "MODELS_DEMO_DIR = PROJECT_ROOT / \"models_demo\"\n",
    "DATA_DEMO_DIR = PROJECT_ROOT / \"data_demo\"\n",
    "\n",
    "# Demo paths\n",
    "DATASET_PATH = DATA_DEMO_DIR / \"emergency_dataset_demo.jsonl\"\n",
    "LORA_ADAPTERS_PATH = MODELS_DEMO_DIR / \"gemma_3n_demo_adapters\"\n",
    "MERGED_MODEL_PATH = MODELS_DEMO_DIR / \"gemma_3n_demo_merged\"\n",
    "\n",
    "# Lightweight Training Hyperparameters for a quick run\n",
    "DEMO_TRAINING_ARGS = {\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_steps\": 1,\n",
    "    \"max_steps\": 2, # CRITICAL: Only 2 steps to run instantly!\n",
    "    \"learning_rate\": 2e-4,\n",
    "    \"logging_steps\": 1,\n",
    "    \"optim\": \"paged_adamw_8bit\",\n",
    "    \"seed\": 3407,\n",
    "    \"report_to\": \"none\",\n",
    "}\n",
    "\n",
    "# Standard model and LoRA config from your project\n",
    "BASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it\"\n",
    "MAX_SEQ_LENGTH = 1024\n",
    "DTYPE = torch.bfloat16 if is_bfloat16_supported() else torch.float16\n",
    "LORA_CONFIG = {\n",
    "    \"r\": 8, # Smaller rank for faster demo\n",
    "    \"lora_alpha\": 16,\n",
    "    \"lora_dropout\": 0,\n",
    "    \"bias\": \"none\",\n",
    "    \"target_modules\": [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "}\n",
    "\n",
    "# --- Create Demo Files and Directories ---\n",
    "DATA_DEMO_DIR.mkdir(exist_ok=True)\n",
    "MODELS_DEMO_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Create the mock dataset file using your provided example\n",
    "with open(DATASET_PATH, \"w\") as f:\n",
    "    f.write('{\"instruction\": \"How do I treat a sprained ankle?\", \"output\": \"1. Rest the ankle and avoid putting weight on it.\\\\n2. Apply a cold pack wrapped in a cloth for 15‚Äì20 minutes at a time, several times a day.\\\\n3. Wrap the ankle with an elastic bandage for compression, but not so tight as to cut off circulation.\\\\n4. Elevate the ankle above heart level when possible to reduce swelling.\\\\n5. Seek medical attention if you cannot walk, the ankle looks deformed, or the pain is severe. Call 911 for emergency care if there is a bone protruding or you suspect a fracture.\"}\\n')\n",
    "    f.write('{\"instruction\": \"If I or someone else has a bee sting, what steps do I follow?\", \"output\": \"1. Use the edge of a credit card or your fingernail to scrape the stinger out gently without squeezing it.\\\\n2. Wash the area with soap and water.\\\\n3. Apply a cold pack to reduce swelling and pain.\\\\n4. Consider taking an over‚Äëthe‚Äëcounter antihistamine for itching or a pain reliever for discomfort.\\\\n5. Watch for signs of an allergic reaction such as difficulty breathing, swelling of the face or throat; call 911 immediately if they occur.\"}\\n')\n",
    "\n",
    "print(\"Demo environment is set up and ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Fine-Tuning (Live Run)\n",
    "\n",
    "This cell executes the **real fine-tuning code** from `train_pipeline.py`. It uses the lightweight configuration, so it will complete in just a few seconds while demonstrating that the training loop, data processing, and model saving logic are all correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 1: Starting QLoRA Fine-tuning (Live Demo Run) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ebe0ab64b5b4241943517dbd6538f52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3080 Ti Laptop GPU. Num GPUs = 1. Max memory: 16.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01c55523db6041ffa441344a1910912d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Making `model.base_model.model.model.language_model` require gradients\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "num_proc must be <= 2. Reducing num_proc to 2 for dataset of size 2.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d1e9c859a34dcea846c59c8e1fd500",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa6aaabf97a54c70b516d3776c142992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Unsloth: Tokenizing [\"text\"] (num_proc=2):   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
      "   \\\\   /|    Num examples = 2 | Num Epochs = 1 | Total steps = 2\n",
      "O^O/ \\_/ \\    Batch size per device = 1 | Gradient accumulation steps = 1\n",
      "\\        /    Data Parallel GPUs = 1 | Total batch size (1 x 1 x 1) = 1\n",
      " \"-____-\"     Trainable parameters = 4,079,616 of 5,443,517,888 (0.07% trained)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:05, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>8.818300</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unsloth: Will smartly offload gradients to save VRAM!\n",
      "\n",
      "Saving final adapter model...\n",
      "\n",
      "LoRA adapters saved to: /app/models_demo/gemma_3n_demo_adapters\n"
     ]
    }
   ],
   "source": [
    "# --- This is the core logic from your project, now with all optimizations ---\n",
    "\n",
    "print(\"--- STAGE 1: Starting QLoRA Fine-tuning (Live Demo Run) ---\")\n",
    "\n",
    "# 1. Load dataset\n",
    "dataset = load_dataset(\"json\", data_files=str(DATASET_PATH), split=\"train\")\n",
    "\n",
    "# 2. Load 4-bit model\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, dtype=DTYPE, load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# 3. Apply LoRA (with Unsloth's gradient checkpointing)\n",
    "model = FastModel.get_peft_model(\n",
    "    model,\n",
    "    **LORA_CONFIG,\n",
    "    use_gradient_checkpointing=\"unsloth\", # CRITICAL: Added from your working file\n",
    ")\n",
    "\n",
    "# 4. Format data\n",
    "def format_chat_template(sample: dict) -> dict:\n",
    "    messages = [{\"role\": \"user\", \"content\": sample[\"instruction\"]}, {\"role\": \"assistant\", \"content\": sample[\"output\"]}]\n",
    "    return {\"text\": tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)}\n",
    "formatted_dataset = dataset.map(format_chat_template, num_proc=os.cpu_count()//2)\n",
    "\n",
    "# 5. Train the model (with packing=True)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    packing=True, # CRITICAL: Added from your working file\n",
    "    max_seq_length=MAX_SEQ_LENGTH,\n",
    "    args=SFTConfig(\n",
    "        output_dir=str(LORA_ADAPTERS_PATH),\n",
    "        bf16=is_bfloat16_supported(),\n",
    "        fp16=not is_bfloat16_supported(),\n",
    "        **DEMO_TRAINING_ARGS\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nSaving final adapter model...\")\n",
    "trainer.model.save_pretrained(str(LORA_ADAPTERS_PATH))\n",
    "tokenizer.save_pretrained(str(LORA_ADAPTERS_PATH)) # –¢–∞–∫–∂–µ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–∫–µ–Ω–∞–π–∑–µ—Ä\n",
    "\n",
    "print(f\"\\nLoRA adapters saved to: {LORA_ADAPTERS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Merging the Model (Live Run)\n",
    "\n",
    "Now, we execute the second part of our training pipeline: merging the trained LoRA adapters into the base model to create a final, standalone artifact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- STAGE 2: Starting Model Merge (Live Demo Run) ---\n",
      "==((====))==  Unsloth 2025.8.1: Fast Gemma3N patching. Transformers: 4.53.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 3080 Ti Laptop GPU. Num GPUs = 1. Max memory: 16.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.7.1+cu126. CUDA: 8.6. CUDA Toolkit: 12.6. Triton: 3.3.1\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.31.post1. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "Unsloth: Gemma3N does not support SDPA - switching to eager!\n",
      "Unsloth: QLoRA and full finetuning all not selected. Switching to 16bit LoRA.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e356808505204873a49568d865f98145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "/opt/venv/lib/python3.11/site-packages/accelerate/utils/modeling.py:1582: UserWarning: Current model requires 38644.0 bytes of buffer for offloaded layers, which seems does not fit any GPU's remaining memory. If you are experiencing a OOM later, please consider using offload_buffers=True.\n",
      "  warnings.warn(\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1b5110e6164335b87d8dadc0078d23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final merged model saved to: /app/models_demo/gemma_3n_demo_merged\n"
     ]
    }
   ],
   "source": [
    "# --- This is the merging logic from src/train_pipeline.py ---\n",
    "\n",
    "print(\"--- STAGE 2: Starting Model Merge (Live Demo Run) ---\")\n",
    "\n",
    "# Clear memory first\n",
    "del model, trainer\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load base model in full precision\n",
    "model, tokenizer = FastModel.from_pretrained(\n",
    "    model_name=BASE_MODEL_NAME, max_seq_length=MAX_SEQ_LENGTH, dtype=DTYPE, load_in_4bit=False,\n",
    ")\n",
    "\n",
    "# Attach adapters and merge\n",
    "model = PeftModel.from_pretrained(model, str(LORA_ADAPTERS_PATH))\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# Save the final merged model\n",
    "model.save_pretrained(str(MERGED_MODEL_PATH))\n",
    "tokenizer.save_pretrained(str(MERGED_MODEL_PATH))\n",
    "\n",
    "print(f\"Final merged model saved to: {MERGED_MODEL_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Inference with the Merged Model (Live Run)\n",
    "\n",
    "The pipeline is complete! We now have a fine-tuned model saved in `models_demo/`. Let's run a live inference call.\n",
    "\n",
    "**Note:** Since we only trained for 2 steps on 2 examples, the model's output will be random and nonsensical. **This is expected and correct**, as it proves the model can be loaded and can generate text end-to-end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- INFERENCE (Live Demo Run) ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "301bf990c59b4d73a63f1eb3744a264b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bos><start_of_turn>user\n",
      "You are a helpful assistant for emergency situations.\n",
      "\n",
      "How do I treat a sprained ankle?<end_of_turn>\n",
      "<start_of_turn>model\n",
      "Okay, I understand you're looking for information on how to treat a sprained ankle.  **I must preface this with a very important disclaimer:**  **I am an AI and cannot provide medical advice. This information is for general knowledge and\n",
      "\n",
      "\n",
      "Inference call complete.\n"
     ]
    }
   ],
   "source": [
    "# --- This is the inference logic from src/inference.py ---\n",
    "\n",
    "print(\"--- INFERENCE (Live Demo Run) ---\")\n",
    "\n",
    "# 1. Load the merged model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(str(MERGED_MODEL_PATH), torch_dtype=DTYPE, device_map=\"auto\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(str(MERGED_MODEL_PATH))\n",
    "\n",
    "# 2. Prepare prompt\n",
    "system_prompt = \"You are a helpful assistant for emergency situations.\"\n",
    "user_prompt = \"How do I treat a sprained ankle?\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt},\n",
    "]\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 3. Generate response\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "_ = model.generate(input_ids, streamer=text_streamer, max_new_tokens=50)\n",
    "\n",
    "print(\"\\n\\nInference call complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Cleanup\n",
    "\n",
    "To keep the project directory clean, this final cell removes all temporary files and folders created during the demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Demo directories successfully cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Clean up the directories created for this demo\n",
    "try:\n",
    "    shutil.rmtree(MODELS_DEMO_DIR)\n",
    "    shutil.rmtree(DATA_DEMO_DIR)\n",
    "    print(\"‚úÖ Demo directories successfully cleaned up.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error during cleanup: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Converting the Model to GGUF (Demonstration)\n",
    "\n",
    "The final step in our pipeline is to convert the fine-tuned model into the highly efficient GGUF format. This makes the model portable and allows it to run on a wide range of hardware (including CPUs) using tools like `llama.cpp`.\n",
    "\n",
    "To keep our main training environment clean, this process uses a separate, lightweight Docker image (`gguf-converter`) which contains all the necessary compilation tools.\n",
    "\n",
    "Below, we simulate this conversion process. The actual commands are shown for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- SIMULATING GGUF CONVERSION ---\n",
      "Input model directory: /app/models_demo/gemma_3n_demo_merged\n",
      "Output directory for GGUF files: /app/models_demo/gguf\n",
      "\n",
      "Step 1: Converting to F16 GGUF...\n",
      "Step 2: Quantizing to Q4_K_M...\n",
      "\n",
      "‚úÖ Simulation complete. Dummy GGUF file created at: /app/models_demo/gguf/gemma-finetuned-Q4_K_M.gguf\n",
      "\n",
      "--- SIMULATING GGUF INFERENCE TEST ---\n",
      "Loading dummy model from /app/models_demo/gguf/gemma-finetuned-Q4_K_M.gguf...\n",
      "> User Prompt: How do I treat a sprained ankle?\n",
      "\n",
      "< Model Response (simulated from GGUF):\n",
      "1. Rest the ankle.\n",
      "2. Apply a cold pack.\n"
     ]
    }
   ],
   "source": [
    "# --- This cell simulates the GGUF conversion and testing process ---\n",
    "\n",
    "# In a real run, you would execute these commands in your terminal:\n",
    "# 1. Build the converter image:\n",
    "#    docker build -t gguf-converter -f Dockerfile.convert .\n",
    "#\n",
    "# 2. Run the conversion script:\n",
    "#    docker run -it --rm -v \"$(pwd)/models_demo/gemma_3n_demo_merged:/app/model_input:ro\" -v \"$(pwd)/models_demo/gguf:/app/model_output\" -v \"$(pwd)/scripts/convert_to_gguf.py:/app/convert_to_gguf.py\" gguf-converter python /app/convert_to_gguf.py\n",
    "\n",
    "GGUF_MODELS_PATH = MODELS_DEMO_DIR / \"gguf\"\n",
    "GGUF_MODELS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"--- SIMULATING GGUF CONVERSION ---\")\n",
    "print(f\"Input model directory: {MERGED_MODEL_PATH}\")\n",
    "print(f\"Output directory for GGUF files: {GGUF_MODELS_PATH}\")\n",
    "\n",
    "print(\"\\nStep 1: Converting to F16 GGUF...\")\n",
    "import time\n",
    "time.sleep(1) # Simulate work\n",
    "print(\"Step 2: Quantizing to Q4_K_M...\")\n",
    "time.sleep(1)\n",
    "\n",
    "# Create a dummy GGUF file to show the result\n",
    "dummy_gguf_file = GGUF_MODELS_PATH / \"gemma-finetuned-Q4_K_M.gguf\"\n",
    "with open(dummy_gguf_file, \"w\") as f:\n",
    "    f.write(\"This is a dummy GGUF file.\")\n",
    "\n",
    "print(f\"\\n‚úÖ Simulation complete. Dummy GGUF file created at: {dummy_gguf_file}\")\n",
    "\n",
    "print(\"\\n--- SIMULATING GGUF INFERENCE TEST ---\")\n",
    "# In a real run, you would execute:\n",
    "#    docker run -it --rm -v \"$(pwd)/models_demo/gguf:/app/models:ro\" -v \"$(pwd)/scripts/inference_gguf.py:/app/inference_gguf.py\" gguf-converter python /app/inference_gguf.py\n",
    "\n",
    "print(f\"Loading dummy model from {dummy_gguf_file}...\")\n",
    "time.sleep(0.5)\n",
    "print(\"> User Prompt: How do I treat a sprained ankle?\")\n",
    "print(\"\\n< Model Response (simulated from GGUF):\")\n",
    "print(\"1. Rest the ankle.\\n2. Apply a cold pack.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All demo directories successfully cleaned up.\n"
     ]
    }
   ],
   "source": [
    "# Clean up all directories created for this demo\n",
    "try:\n",
    "    if os.path.exists(MODELS_DEMO_DIR):\n",
    "        shutil.rmtree(MODELS_DEMO_DIR)\n",
    "    if os.path.exists(DATA_DEMO_DIR):\n",
    "        shutil.rmtree(DATA_DEMO_DIR)\n",
    "    print(\"‚úÖ All demo directories successfully cleaned up.\")\n",
    "except OSError as e:\n",
    "    print(f\"Error during cleanup: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "databundleVersionId": 12693789,
     "sourceId": 105267,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
